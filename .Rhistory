## Our objective function
objfun <- function(beta,y,X) {
return (sum((y-X%*%beta)^2))
# equivalently, if we want to use matrix algebra:
# return ( crossprod(y-X%*%beta) )
}
## Gradient of our objective function
gradient <- function(beta,Y,X) {
return ( as.vector(-2*t(X)%*%(Y-X%*%beta)) )
}
## initial values
beta0 <- runif(dim(X)[2]) #start at uniform random numbers equal to number of coefficients
## Algorithm parameters
options <- list("algorithm"="NLOPT_LD_LBFGS","xtol_rel"=1.0e-6,"maxeval"=1e3)
## Optimize!
betaOLSgrad <- nloptr( x0=beta0,eval_f=objfun,eval_grad_f=gradient,opts=options,Y=Y,X=X)
library(nloptr)
## Our objective function
objfun <- function(beta,Y,X) {
return (sum((Y-X%*%beta)^2))
# equivalently, if we want to use matrix algebra:
# return ( crossprod(y-X%*%beta) )
}
## Gradient of our objective function
gradient <- function(beta,Y,X) {
return ( as.vector(-2*t(X)%*%(Y-X%*%beta)) )
}
## initial values
beta0 <- runif(dim(X)[2]) #start at uniform random numbers equal to number of coefficients
## Algorithm parameters
options <- list("algorithm"="NLOPT_LD_LBFGS","xtol_rel"=1.0e-6,"maxeval"=1e3)
## Optimize!
betaOLSgrad <- nloptr( x0=beta0,eval_f=objfun,eval_grad_f=gradient,opts=options,Y=Y,X=X)
options <- list("algorithm"="NLOPT_LN_NELDERMEAD","xtol_rel"=1.0e-6,"maxeval"=1e3)
betaOLSnm <- nloptr( x0=beta0,eval_f=objfun,eval_grad_f=gradient,opts=options,Y=Y,X=X)
objfun  <- function(theta,Y,X) {
# need to slice our parameter vector into beta and sigma components
beta    <- theta[1:(length(theta)-1)]
sig     <- theta[length(theta)]
# write objective function as *negative* log likelihood (since NLOPT minimizes)
loglike <- -sum( -.5*(log(2*pi*(sig^2)) + ((Y-X%*%beta)/sig)^2) )
return (loglike)
}
## read in the data
## initial values
theta0 <- runif(dim(X)[2]+1) #start at uniform random numbers equal to number of coefficients
theta0 <- append(as.vector(summary(lm(Sepal.Length~Sepal.Width+Petal.Length+Petal.Width+Species,data=iris))$coefficients[,1]),runif(1))
## Algorithm parameters
options <- list("algorithm"="NLOPT_LN_NELDERMEAD","xtol_rel"=1.0e-6,"maxeval"=1e4)
## Optimize!
betaMLE <- nloptr( x0=theta0,eval_f=objfun,opts=options,Y=Y,X=X)
objfun  <- function(theta,Y,X) {
# need to slice our parameter vector into beta and sigma components
beta1    <- theta[1:(length(theta)-1)]
sig     <- theta[length(theta)]
# write objective function as *negative* log likelihood (since NLOPT minimizes)
loglike <- -sum( -.5*(log(2*pi*(sig^2)) + ((Y-X%*%beta1)/sig)^2) )
return (loglike)
}
## read in the data
## initial values
theta0 <- runif(dim(X)[2]+1) #start at uniform random numbers equal to number of coefficients
## Algorithm parameters
options <- list("algorithm"="NLOPT_LN_NELDERMEAD","xtol_rel"=1.0e-6,"maxeval"=1e4)
## Optimize!
betaMLE <- nloptr( x0=theta0,eval_f=objfun,opts=options,Y=Y,X=X)
library(stargazer)
stargazer(betaOLSxxxy,betaOLSnm,betaOLSgrad,betaMLE)
stargazer(betaOLSlm)
betaOLSlm = lm(Y ~ X -1)
stargazer(betaOLSlm)
source('~/DScourseS20/TwitterProject/get tweets 2.R', echo=TRUE)
source('~/DScourseS20/TwitterProject/get tweets 2.R', echo=TRUE)
source('~/DScourseS20/TwitterProject/get tweets 2.R', echo=TRUE)
library(rtweet)
#Load Candidates
load("~/DScourseS20/TwitterProject/Candidates.Rda")
#Set date range from target start date to date after target end date
dateStart = as.Date("2020-03-25")
dateEnd = as.Date("2020-03-26")
for (row in 1:nrow(tweets)){
if(tweets$dropout[row] > dateStart){
candidate_query = paste0(tweets$handle[row]," -filter:retweets -filter:replies since:",dateStart," until:",dateEnd)
candidate_tweets <- search_tweets(candidate_query, n = 100000, retryonratelimit = TRUE)
save(candidate_tweets,file=paste0("~/DScourseS20/TwitterProject/RawTweets/",tweets$tag[row],"_",dateStart,".Rda"))
}
}
source('~/DScourseS20/TwitterProject/get tweets 2.R', echo=TRUE)
source('~/DScourseS20/TwitterProject/get tweets 2.R', echo=TRUE)
source('~/DScourseS20/TwitterProject/get tweets 2.R', echo=TRUE)
source('~/DScourseS20/TwitterProject/get tweets 2.R', echo=TRUE)
install.packages("mlr")
install.packages("glmnet")
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
library(mlr)
library(glmnet)
housing <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data")
names(housing) <- c("crim","zn","indus","chas","nox","rm","age","dis","rad","tax","ptratio","b","lstat","medv")
housing $lmedv <- log ( housing $ medv)
housing $medv <- NULL # drop median value
formula <- as.formula (lmedv ~ .^3 +
poly (crim , 6) +
poly (zn , 6) +
poly (indus , 6) +
poly (nox , 6) +
poly (rm , 6) +
poly (age , 6) +
poly (dis , 6) +
poly (rad , 6) +
poly (tax , 6) +
poly (ptratio , 6) +
poly (b, 6) +
poly (lstat , 6))
mod _ matrix <- data . frame ( model . matrix ( formula , housing ))
mod_matrix <- data.frame(model.matrix( formula , housing ))
mod_matrix[, 1] = housing$lmedv
colnames(mod_matrix )[1] = "lmedv" #make sure to rename it otherwise MLR won â€™t find it
head(mod_matrix ) #just make sure everything is hunky -dory
n <- nrow (mod_matrix )
train <- sample (n, size = .8*n)
test <- setdiff (1:n, train)
housing.train <- mod _ matrix [train ,]
housing.train <- mod_matrix [train ,]
housing.test <- mod_matrix [test , ]
#LASSO
theTask <- makeRegrTask(id = "taskname", data = housing.train, target = "lmedv")
resampleStrat <- makeResampleDesc(method = "CV", iters = 6)
predAlg <- makeLearner("regr.glmnet")
# Search over penalty parameter lambda and force elastic net parameter to be 1 (LASSO)
modelParams <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=1,upper=1))
# Take 50 random guesses at lambda within the interval I specified above
tuneMethod <- makeTuneControlRandom(maxit = 50L)
# Do the tuning
tunedModel <- tuneParams(learner = predAlg,
task = theTask,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = modelParams,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
predAlg <- setHyperPars(learner=predAlg, par.vals = tunedModel$x)
# Verify performance on cross validated sample sets
resample(predAlg,theTask,resampleStrat,measures=list(rmse))
# Train the final model
finalModel <- train(learner = predAlg, task = theTask)
# Predict in test set!
prediction <- predict(finalModel, newdata = housing.test)
print(head(prediction$data))
print(get.rmse(prediction$data$truth,prediction$data$response))
# Trained parameter estimates
getLearnerModel(finalModel)$beta
library(mlr)
library(glmnet)
library(tidyverse)
library(magrittr)
library(mlr)
install.packages("mlr")
install.packages("glmnet")
library(mlr)
library(glmnet)
#LASSO
theTask <- makeRegrTask(id = "taskname", data = housing.train, target = "lmedv")
resampleStrat <- makeResampleDesc(method = "CV", iters = 6)
predAlg <- makeLearner("regr.glmnet")
library(glmnet)
install.packages("mlr")
install.packages("glmnet")
library(mlr)
library(glmnet)
#LASSO
theTask <- makeRegrTask(id = "taskname", data = housing.train, target = "lmedv")
resampleStrat <- makeResampleDesc(method = "CV", iters = 6)
predAlg <- makeLearner("regr.glmnet")
# Search over penalty parameter lambda and force elastic net parameter to be 1 (LASSO)
modelParams <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=1,upper=1))
# Take 50 random guesses at lambda within the interval I specified above
tuneMethod <- makeTuneControlRandom(maxit = 50L)
# Do the tuning
tunedModel <- tuneParams(learner = predAlg,
task = theTask,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = modelParams,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
predAlg <- setHyperPars(learner=predAlg, par.vals = tunedModel$x)
# Verify performance on cross validated sample sets
resample(predAlg,theTask,resampleStrat,measures=list(rmse))
# Train the final model
finalModel <- train(learner = predAlg, task = theTask)
# Predict in test set!
prediction <- predict(finalModel, newdata = housing.test)
print(head(prediction$data))
print(get.rmse(prediction$data$truth,prediction$data$response))
# Trained parameter estimates
getLearnerModel(finalModel)$beta
View(resampleStrat)
View(resampleStrat)
View(finalModel)
View(finalModel)
finalModel[["learner.model"]][["lambda"]]
View(modelParams)
View(tuneMethod)
View(tuneMethod)
print(get.rmse(prediction$data$truth,prediction$data$response))
library(tidyverse)
library(magrittr)
print(get.rmse(prediction$data$truth,prediction$data$response))
theTask <- makeRegrTask(id = "taskname", data = housing.train, target = "lmedv")
resampleStrat <- makeResampleDesc(method = "CV", iters = 6)
predAlg <- makeLearner("regr.glmnet")
# Search over penalty parameter lambda and force elastic net parameter to be 1 (LASSO)
modelParams <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=1,upper=1))
# Take 50 random guesses at lambda within the interval I specified above
tuneMethod <- makeTuneControlRandom(maxit = 50L)
# Do the tuning
tunedModel <- tuneParams(learner = predAlg,
task = theTask,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = modelParams,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
predAlg <- setHyperPars(learner=predAlg, par.vals = tunedModel$x)
# Verify performance on cross validated sample sets
resample(predAlg,theTask,resampleStrat,measures=list(rmse))
# Train the final model
finalModel <- train(learner = predAlg, task = theTask)
# Predict in test set!
prediction <- predict(finalModel, newdata = housing.test)
print(head(prediction$data))
print(get.rmse(prediction$data$truth,prediction$data$response))
print(rmse(prediction$data$truth,prediction$data$response))
get.rmse <- function(y1,y2){
return(sqrt(mean((y1-y2)^2)))
}
print(head(prediction$data))
print(get.rmse(prediction$data$truth,prediction$data$response))
View(prediction)
View(prediction)
View(housing.train)
View(housing.train)
print(get.rmse(prediction$data$truth,housing.training$lmedv))
print(get.rmse(prediction$data$truth,housing.train$lmedv))
print(get.rmse(prediction$data$truth,housing.test$lmedv))
print(get.rmse(prediction$data$response,housing.test$lmedv))
# Search over penalty parameter lambda and force elastic net parameter to be 0 (ridge)
modelParams <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=0,upper=0))
# Do the tuning again
tunedModel <- tuneParams(learner = predAlg,
task = theTask,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = modelParams,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
predAlg <- setHyperPars(learner=predAlg, par.vals = tunedModel$x)
# Verify performance on cross validated sample sets
resample(predAlg,theTask,resampleStrat,measures=list(rmse))
# Train the final model
finalModel <- train(learner = predAlg, task = theTask)
# Predict in test set!
prediction <- predict(finalModel, newdata = housing.test)
# Print RMSE
print(get.rmse(prediction$data$truth,prediction$data$response))
# Trained parameter estimates
getLearnerModel(finalModel)$beta
print(get.rmse(prediction$data$truth,prediction$data$response))
print(get.rmse(prediction$data$truth,housing.train$lmedv))
View(prediction)
View(prediction)
View(tunedModel)
View(tunedModel)
View(finalModel)
View(finalModel)
View(finalModel)
View(finalModel)
View(tuneMethod)
View(tuneMethod)
View(tunedModel)
tunedModel[["y"]]
View(prediction)
View(prediction)
View(modelParams)
View(modelParams)
#LASSO
theTask <- makeRegrTask(id = "taskname", data = housing.train, target = "lmedv")
resampleStrat <- makeResampleDesc(method = "CV", iters = 6)
predAlg <- makeLearner("regr.glmnet")
# Search over penalty parameter lambda and force elastic net parameter to be 1 (LASSO)
modelParams <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=1,upper=1))
# Take 50 random guesses at lambda within the interval I specified above
tuneMethod <- makeTuneControlRandom(maxit = 50L)
# Do the tuning
tunedModel <- tuneParams(learner = predAlg,
task = theTask,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = modelParams,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
predAlg <- setHyperPars(learner=predAlg, par.vals = tunedModel$x)
# Verify performance on cross validated sample sets
resample(predAlg,theTask,resampleStrat,measures=list(rmse))
# Train the final model
finalModel <- train(learner = predAlg, task = theTask)
# Predict in test set!
prediction <- predict(finalModel, newdata = housing.test)
get.rmse <- function(y1,y2){
return(sqrt(mean((y1-y2)^2)))
}
print(head(prediction$data))
print(get.rmse(prediction$data$truth,prediction$data$response))
print(get.rmse(prediction$data$truth,housing.train$lmedv))
# Trained parameter estimates
getLearnerModel(finalModel)$beta
View(tunedModel)
View(tunedModel)
tunedModel[["y"]]
View(modelParams)
View(modelParams)
getLearnerModel(finalModel)$beta
View(finalModel)
View(finalModel)
finalModel[["learner"]][["par.vals"]][["lambda"]]
#Elastic Net
# Search over penalty parameter lambda and force elastic net parameter to be 0 (ridge)
modelParams <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=0,upper=1))
# Do the tuning again
tunedModel <- tuneParams(learner = predAlg,
task = theTask,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = modelParams,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
predAlg <- setHyperPars(learner=predAlg, par.vals = tunedModel$x)
# Verify performance on cross validated sample sets
resample(predAlg,theTask,resampleStrat,measures=list(rmse))
# Train the final model
finalModel <- train(learner = predAlg, task = theTask)
# Predict in test set!
prediction <- predict(finalModel, newdata = housing.test)
# Print RMSE
print(get.rmse(prediction$data$truth,prediction$data$response))
# Trained parameter estimates
getLearnerModel(finalModel)$beta
#RIDGE
# Search over penalty parameter lambda and force elastic net parameter to be 0 (ridge)
modelParams <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=0,upper=0))
# Do the tuning again
tunedModel <- tuneParams(learner = predAlg,
task = theTask,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = modelParams,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
predAlg <- setHyperPars(learner=predAlg, par.vals = tunedModel$x)
# Verify performance on cross validated sample sets
resample(predAlg,theTask,resampleStrat,measures=list(rmse))
# Train the final model
finalModel <- train(learner = predAlg, task = theTask)
# Predict in test set!
prediction <- predict(finalModel, newdata = housing.test)
# Print RMSE
print(get.rmse(prediction$data$truth,prediction$data$response))
#Elastic Net
# Search over penalty parameter lambda and force elastic net parameter to be 0 (ridge)
modelParams <- makeParamSet(makeNumericParam("lambda",lower=0,upper=1),makeNumericParam("alpha",lower=0,upper=1))
# Do the tuning again
tunedModel <- tuneParams(learner = predAlg,
task = theTask,
resampling = resampleStrat,
measures = rmse,       # RMSE performance measure, this can be changed to one or many
par.set = modelParams,
control = tuneMethod,
show.info = TRUE)
# Apply the optimal algorithm parameters to the model
predAlg <- setHyperPars(learner=predAlg, par.vals = tunedModel$x)
# Verify performance on cross validated sample sets
resample(predAlg,theTask,resampleStrat,measures=list(rmse))
# Train the final model
finalModel <- train(learner = predAlg, task = theTask)
# Predict in test set!
prediction <- predict(finalModel, newdata = housing.test)
# Print RMSE
print(get.rmse(prediction$data$truth,prediction$data$response))
# Trained parameter estimates
getLearnerModel(finalModel)$beta
print(get.rmse(prediction$data$truth,prediction$data$response))
View(tunedModel)
View(tunedModel)
tunedModel[["y"]]
View(finalModel)
View(finalModel)
View(prediction)
View(prediction)
View(modelParams)
View(modelParams)
finalModel[["learner"]][["par.vals"]][["lambda"]]
finalModel[["learner"]][["par.vals"]][["alpha"]]
install.packages("rpart")
install.packages("e1071")
install.packages("kknn")
install.packages("nnet")
set.seed(100)
income <- read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data")
names(income) <- c("age","workclass","fnlwgt","education","education.num","marital.status","occupation","relationship","race","sex","capital.gain","capital.loss","hours","native.country","high.earner")
# From UC Irvine's website (http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names)
#   age: continuous.
#   workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.
#   fnlwgt: continuous.
#   education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.
#   education-num: continuous.
#   marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
#   occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.
#   relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.
#   race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.
#   sex: Female, Male.
#   capital-gain: continuous.
#   capital-loss: continuous.
#   hours-per-week: continuous.
#   native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.
######################
# Clean up the data
######################
# Drop unnecessary columns
income$native.country <- NULL
income$fnlwgt         <- NULL
# Make sure continuous variables are coded as such
income$age            <- as.numeric(income$age)
income$hours          <- as.numeric(income$hours)
income$education.num  <- as.numeric(income$education.num)
income$capital.gain   <- as.numeric(income$capital.gain)
income$capital.loss   <- as.numeric(income$capital.loss)
# Combine levels of categorical variables that currently have too many levels
levels(income$education) <- list(Advanced = c("Masters,","Doctorate,","Prof-school,"), Bachelors = c("Bachelors,"), "Some-college" = c("Some-college,","Assoc-acdm,","Assoc-voc,"), "HS-grad" = c("HS-grad,","12th,"), "HS-drop" = c("11th,","9th,","7th-8th,","1st-4th,","10th,","5th-6th,","Preschool,"))
levels(income$marital.status) <- list(Married = c("Married-civ-spouse,","Married-spouse-absent,","Married-AF-spouse,"), Divorced = c("Divorced,","Separated,"), Widowed = c("Widowed,"), "Never-married" = c("Never-married,"))
levels(income$race) <- list(White = c("White,"), Black = c("Black,"), Asian = c("Asian-Pac-Islander,"), Other = c("Other,","Amer-Indian-Eskimo,"))
levels(income$workclass) <- list(Private = c("Private,"), "Self-emp" = c("Self-emp-not-inc,","Self-emp-inc,"), Gov = c("Federal-gov,","Local-gov,","State-gov,"), Other = c("Without-pay,","Never-worked,","?,"))
levels(income$occupation) <- list("Blue-collar" = c("?,","Craft-repair,","Farming-fishing,","Handlers-cleaners,","Machine-op-inspct,","Transport-moving,"), "White-collar" = c("Adm-clerical,","Exec-managerial,","Prof-specialty,","Sales,","Tech-support,"), Services = c("Armed-Forces,","Other-service,","Priv-house-serv,","Protective-serv,"))
# Break up the data:
n <- nrow(income)
train <- sample(n, size = .8*n)
test  <- setdiff(1:n, train)
income.train <- income[train,]
income.test  <- income[test, ]
set.seed(100)
income <- read.table("C:\Users\kmmcg\Downloads\adult.data")
income <- read.table("C:/Users/kmmcg/Downloads/adult.data")
names(income) <- c("age","workclass","fnlwgt","education","education.num","marital.status","occupation","relationship","race","sex","capital.gain","capital.loss","hours","native.country","high.earner")
# From UC Irvine's website (http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names)
#   age: continuous.
#   workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.
#   fnlwgt: continuous.
#   education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.
#   education-num: continuous.
#   marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
#   occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.
#   relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.
#   race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.
#   sex: Female, Male.
#   capital-gain: continuous.
#   capital-loss: continuous.
#   hours-per-week: continuous.
#   native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.
######################
# Clean up the data
######################
# Drop unnecessary columns
income$native.country <- NULL
income$fnlwgt         <- NULL
# Make sure continuous variables are coded as such
income$age            <- as.numeric(income$age)
income$hours          <- as.numeric(income$hours)
income$education.num  <- as.numeric(income$education.num)
income$capital.gain   <- as.numeric(income$capital.gain)
income$capital.loss   <- as.numeric(income$capital.loss)
# Combine levels of categorical variables that currently have too many levels
levels(income$education) <- list(Advanced = c("Masters,","Doctorate,","Prof-school,"), Bachelors = c("Bachelors,"), "Some-college" = c("Some-college,","Assoc-acdm,","Assoc-voc,"), "HS-grad" = c("HS-grad,","12th,"), "HS-drop" = c("11th,","9th,","7th-8th,","1st-4th,","10th,","5th-6th,","Preschool,"))
levels(income$marital.status) <- list(Married = c("Married-civ-spouse,","Married-spouse-absent,","Married-AF-spouse,"), Divorced = c("Divorced,","Separated,"), Widowed = c("Widowed,"), "Never-married" = c("Never-married,"))
levels(income$race) <- list(White = c("White,"), Black = c("Black,"), Asian = c("Asian-Pac-Islander,"), Other = c("Other,","Amer-Indian-Eskimo,"))
levels(income$workclass) <- list(Private = c("Private,"), "Self-emp" = c("Self-emp-not-inc,","Self-emp-inc,"), Gov = c("Federal-gov,","Local-gov,","State-gov,"), Other = c("Without-pay,","Never-worked,","?,"))
levels(income$occupation) <- list("Blue-collar" = c("?,","Craft-repair,","Farming-fishing,","Handlers-cleaners,","Machine-op-inspct,","Transport-moving,"), "White-collar" = c("Adm-clerical,","Exec-managerial,","Prof-specialty,","Sales,","Tech-support,"), Services = c("Armed-Forces,","Other-service,","Priv-house-serv,","Protective-serv,"))
# Break up the data:
n <- nrow(income)
train <- sample(n, size = .8*n)
test  <- setdiff(1:n, train)
income.train <- income[train,]
income.test  <- income[test, ]
library(rpart)
library(e1071)
library(kknn)
library(nnet)
library(mlr)
library(glmnet)
theTask <- makeRegrTask(id = "taskname", data = income.train, target = "high.earner")
theTask <- makeRegrTask(id = "taskname", data = income.train, target = as.numeric("high.earner")
)
theTask <- makeRegrTask(id = "taskname", data = income.train, target = "high.earner")
View(income.train)
View(income.train)
